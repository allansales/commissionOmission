{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "calmca - agaist marijuana legalization\n",
    "mpp - pro marijuana legalization\n",
    "prochoice - pro abortion\n",
    "abort73 - against abortion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_json = pd.read_json(\"crawlers/procon/procon.json\")\n",
    "\n",
    "def format_args_row(row):\n",
    "    df = row.apply(pd.Series).T\n",
    "    df.url = df.url[0]\n",
    "    df.topic = df.topic[0]\n",
    "    return(df)\n",
    "\n",
    "arg_df_series = args_json.apply(format_args_row, axis=1)\n",
    "args_df = pd.concat(arg_df_series.values.tolist(),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orientation(pro_con):\n",
    "    if \"Pro\" in pro_con:\n",
    "        return \"Pro\"\n",
    "    return \"Con\"\n",
    "\n",
    "args_df = args_df.assign(argument_orient = args_df.apply(lambda row: orientation(row.pro_con), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert from json to dataframe and add the topic and the source stance information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "def create_df(path_list, topic, source_stance):\n",
    "    glob_data = []\n",
    "    for path in path_list:\n",
    "        for file in glob.glob(path):\n",
    "            with open(file) as json_file:\n",
    "                data = json.load(json_file)\n",
    "                glob_data.append(data)\n",
    "\n",
    "        df = pd.DataFrame(glob_data)\n",
    "        df['text_topic'] = topic\n",
    "        df['text_source_stance'] = source_stance\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [\"../../news-please-repo/data/2020/02/06/abort73.com/*.json\"]\n",
    "topic = \"Abortion\"\n",
    "source_stance = \"Con\"\n",
    "\n",
    "abort73 = create_df(path_list, topic, source_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [\"../../news-please-repo/data/2020/02/06/mpp.org/*.json\",\"../../news-please-repo/data/2020/02/06/blog.mpp.org/*.json\"]\n",
    "topic = \"Marijuana\"\n",
    "source_stance = \"Pro\"\n",
    "\n",
    "mpp = create_df(path_list, topic, source_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [\"../../news-please-repo/data/2020/02/06/prochoiceamerica.org/*.json\"]\n",
    "topic = \"Abortion\"\n",
    "source_stance = \"Pro\"\n",
    "\n",
    "prochoice = create_df(path_list, topic, source_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [\"../../news-please-repo/data/2020/02/06/calmca.org/*.json\"]\n",
    "topic = \"Marijuana\"\n",
    "source_stance = \"Con\"\n",
    "\n",
    "calmca = create_df(path_list, topic, source_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df = pd.concat([abort73, prochoice, mpp, calmca], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the columns of dataframes to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['authors', 'date_modify', 'date_publish', 'title', 'source_domain', 'text', 'url', 'text_topic', 'text_source_stance']\n",
    "document_df = pd.DataFrame(document_df, columns=columns)\n",
    "document_df = document_df.rename(columns={\"authors\":\"text_authors\", \"title\":\"text_title\", \"source_domain\":\"text_source_domain\", \"url\":\"text_url\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing document's and argument's text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Removing rows which text contains None from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_df = args_df[~args_df.argument.isna()]\n",
    "document_df = document_df[~document_df.text.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords and measuring the resulting text length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords') \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess_sentence(str_array):\n",
    "    new_sentences = []\n",
    "    sentences_size = []\n",
    "    for text in str_array:\n",
    "        new_sentence = \"\"\n",
    "        for word in text.split():\n",
    "            if word not in stop_words:\n",
    "                new_sentence += \" \" + word.lower()\n",
    "        sentences_size.append(new_sentence.count(\" \"))\n",
    "        new_sentences.append(new_sentence.strip())\n",
    "        \n",
    "    return(new_sentences, sentences_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_df['argument_processed'], args_df['argument_processed_size']  = preprocess_sentence(args_df.argument)\n",
    "document_df['text_processed'], document_df['text_processed_size'] = preprocess_sentence(document_df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge documents and arguments by topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args_docs = args_df.merge(document_df, left_on=['topic'], right_on=['text_topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraphs preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_json = pd.read_json(\"crawlers/procon/wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add preprocess_sentence in the paragraphs processing\n",
    "def format_wiki_row(row):\n",
    "    df = row.apply(pd.Series).T\n",
    "    df.source = df.source[0]\n",
    "    df.title = df.title[0]\n",
    "    df.topic = df.topic[0]\n",
    "    df.stance = df.stance[0]\n",
    "    df.url = df.url[0]\n",
    "    return(df)\n",
    "\n",
    "wiki_df_series = wiki_json.apply(format_wiki_row, axis=1)\n",
    "wiki_df = pd.concat(wiki_df_series.values.tolist(),ignore_index=True)\n",
    "wiki_df = wiki_df.rename(columns={\"topic\":\"text_topic\", \"title\":\"text_title\", \"content\":\"text\", \"source\":\"text_source_domain\", \"stance\":\"text_source_stance\", \"url\":\"text_url\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_prg = args_df.merge(wiki_df, left_on=['topic'], right_on=['text_topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('data/model.bin', binary=True)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "tqdm.pandas()\n",
    "\n",
    "def wmd_computation(df):\n",
    "    df['wmd'] = df.progress_apply(lambda x: model.wmdistance(x.argument_processed.split(), x.text_processed.split()), axis=1)\n",
    "    df = df.query(\"wmd != \"+str(math.inf))\n",
    "    df = df.assign(wmd_norm = (df.wmd - df.wmd.min()) / (df.wmd.max() - df.wmd.min()))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args_prg_dist = wmd_computation(args_prg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args_prg_dist.to_csv(\"../data/args_prg_dist.csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_prg_dist = pd.read_csv(\"data/args_prg_dist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampled_data = args_docs.sample(4000)\n",
    "#args_docs_dist = wmd_computation(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args_docs_dist.to_csv(\"../data/sampled_args_docs_dist.csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_docs_dist = pd.read_csv(\"../data/sampled_args_docs_dist.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import permutation_test\n",
    "\n",
    "def weat(dist_df):\n",
    "\n",
    "    # mean distance of each text to each argument (argument - con/pro) set\n",
    "    sim_text_args = dist_df.groupby([\"text\",\"text_topic\",\"text_title\",\"text_source_domain\",\"argument_orient\",\"text_source_stance\"]).wmd_norm.agg(['mean']).reset_index()\n",
    "    sim_text_args = sim_text_args.rename(columns={\"mean\":\"mean_dist\"})\n",
    "\n",
    "    # difference of distance of each paragraph to the attribute sets\n",
    "    con_args = sim_text_args[sim_text_args.argument_orient == \"Con\"]\n",
    "    pro_args = sim_text_args[sim_text_args.argument_orient == \"Pro\"]\n",
    "    sim_text_args = pro_args.merge(con_args, on=[\"text\",\"text_topic\",\"text_title\",\"text_source_domain\",\"text_source_stance\"]).drop(columns=[\"argument_orient_x\", \"argument_orient_y\"])\n",
    "    sim_text_args = sim_text_args.rename(columns={\"mean_dist_x\":\"pro_dist\",\"mean_dist_y\":\"con_dist\"})\n",
    "    sim_text_args[\"diff_dist\"] = sim_text_args.pro_dist - sim_text_args.con_dist\n",
    "\n",
    "    topics = sim_text_args.text_topic.unique()\n",
    "\n",
    "    significance_test_array = []\n",
    "\n",
    "    for topic in topics:\n",
    "        topic_sources = sim_text_args[sim_text_args.text_topic == topic]\n",
    "        sources = topic_sources.text_source_domain.unique()\n",
    "       \n",
    "        for source in sources:\n",
    "            source_a = topic_sources[(topic_sources.text_source_domain == sources[0])]\n",
    "            source_b = topic_sources[(topic_sources.text_source_domain == sources[1])]\n",
    "\n",
    "            p_value = permutation_test(source_b.diff_dist, source_a.diff_dist,\n",
    "                                       method='approximate', num_rounds=1000)\n",
    "            \n",
    "            interpret = source_b.text_source_domain.iloc[0]+\"-Pro x \"+source_a.text_source_domain.iloc[0]+\"-Con\"\n",
    "            if source_b.diff_dist.mean() > source_a.diff_dist.mean():\n",
    "                interpret = source_b.text_source_domain.iloc[0]+\"-Con x \"+source_a.text_source_domain.iloc[0]+\"-Pro\"\n",
    "            \n",
    "            groundtruth = source_b.text_source_domain.iloc[0] + \"-\" + source_b.text_source_stance.iloc[0] + \" x \" + source_a.text_source_domain.iloc[0] +\"-\"+ source_a.text_source_stance.iloc[0]\n",
    "    \n",
    "        significance_test_array.append([topic, p_value, interpret, groundtruth, source_b.diff_dist.mean(), source_a.diff_dist.mean()])\n",
    "    \n",
    "    significance_test_df = pd.DataFrame(significance_test_array, columns=['topic', 'p_value', 'res_interpretation', 'ground_truth','cons_mean_dif', 'rwiki_mean_dif'])\n",
    "    \n",
    "    return(significance_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weat(args_prg_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weat(args_docs_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
