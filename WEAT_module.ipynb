{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "calmca - agaist marijuana legalization\n",
    "mpp - pro marijuana legalization\n",
    "prochoice - pro abortion\n",
    "abort73 - against abortion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_json = pd.read_json(\"crawlers/procon/procon.json\")\n",
    "\n",
    "def format_args_row(row):\n",
    "    df = row.apply(pd.Series).T\n",
    "    df.url = df.url[0]\n",
    "    df.topic = df.topic[0]\n",
    "    return(df)\n",
    "\n",
    "arg_df_series = args_json.apply(format_args_row, axis=1)\n",
    "args_df = pd.concat(arg_df_series.values.tolist(),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orientation(pro_con):\n",
    "    if \"Pro\" in pro_con:\n",
    "        return \"Pro\"\n",
    "    return \"Con\"\n",
    "\n",
    "args_df = args_df.assign(argument_orient = args_df.apply(lambda row: orientation(row.pro_con), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert from json to dataframe and add the topic and the source stance information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "def create_df(path_list, topic, source_stance):\n",
    "    glob_data = []\n",
    "    for path in path_list:\n",
    "        for file in glob.glob(path):\n",
    "            with open(file) as json_file:\n",
    "                data = json.load(json_file)\n",
    "                glob_data.append(data)\n",
    "\n",
    "        df = pd.DataFrame(glob_data)\n",
    "        df['text_topic'] = topic\n",
    "        df['text_source_stance'] = source_stance\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [\"../../news-please-repo/data/2020/02/06/abort73.com/*.json\"]\n",
    "topic = \"Abortion\"\n",
    "source_stance = \"Con\"\n",
    "\n",
    "abort73 = create_df(path_list, topic, source_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [\"../../news-please-repo/data/2020/02/06/mpp.org/*.json\",\"../../news-please-repo/data/2020/02/06/blog.mpp.org/*.json\"]\n",
    "topic = \"Marijuana\"\n",
    "source_stance = \"Pro\"\n",
    "\n",
    "mpp = create_df(path_list, topic, source_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [\"../../news-please-repo/data/2020/02/06/prochoiceamerica.org/*.json\"]\n",
    "topic = \"Abortion\"\n",
    "source_stance = \"Pro\"\n",
    "\n",
    "prochoice = create_df(path_list, topic, source_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [\"../../news-please-repo/data/2020/02/06/calmca.org/*.json\"]\n",
    "topic = \"Marijuana\"\n",
    "source_stance = \"Con\"\n",
    "\n",
    "calmca = create_df(path_list, topic, source_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df = pd.concat([abort73, prochoice, mpp, calmca], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the columns of dataframes to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['authors', 'date_modify', 'date_publish', 'title', 'source_domain', 'text', 'url', 'text_topic', 'text_source_stance']\n",
    "document_df = pd.DataFrame(document_df, columns=columns)\n",
    "document_df = document_df.rename(columns={\"authors\":\"text_authors\", \"title\":\"text_title\", \"source_domain\":\"text_source_domain\", \"url\":\"text_url\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing document's and argument's text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Removing rows which text contains None from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_df = args_df[~args_df.argument.isna()]\n",
    "document_df = document_df[~document_df.text.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords and measuring the resulting text length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords') \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess_sentence(str_array):\n",
    "    new_sentences = []\n",
    "    sentences_size = []\n",
    "    for text in str_array:\n",
    "        new_sentence = \"\"\n",
    "        for word in text.split():\n",
    "            if word not in stop_words:\n",
    "                new_sentence += \" \" + word.lower()\n",
    "        sentences_size.append(new_sentence.count(\" \"))\n",
    "        new_sentences.append(new_sentence.strip())\n",
    "        \n",
    "    return(new_sentences, sentences_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_df['argument_processed'], args_df['argument_processed_size']  = preprocess_sentence(args_df.argument)\n",
    "document_df['text_processed'], document_df['text_processed_size'] = preprocess_sentence(document_df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge documents and arguments by topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args_docs = args_df.merge(document_df, left_on=['topic'], right_on=['text_topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraphs preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_json = pd.read_json(\"crawlers/procon/wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add preprocess_sentence in the paragraphs processing\n",
    "def format_wiki_row(row):\n",
    "    df = row.apply(pd.Series).T\n",
    "    df.source = df.source[0]\n",
    "    df.title = df.title[0]\n",
    "    df.topic = df.topic[0]\n",
    "    df.stance = df.stance[0]\n",
    "    df.url = df.url[0]\n",
    "    return(df)\n",
    "\n",
    "wiki_df_series = wiki_json.apply(format_wiki_row, axis=1)\n",
    "wiki_df = pd.concat(wiki_df_series.values.tolist(),ignore_index=True)\n",
    "wiki_df = wiki_df.rename(columns={\"topic\":\"text_topic\", \"title\":\"text_title\", \"content\":\"text\", \"source\":\"text_source_domain\", \"stance\":\"text_source_stance\", \"url\":\"text_url\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_prg = args_df.merge(wiki_df, left_on=['topic'], right_on=['text_topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debate Reasons Dataset Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_debate_df(path_list, topic):\n",
    "    glob_data = {}\n",
    "    for path in path_list:\n",
    "        for file in glob.glob(path):\n",
    "            with open(file, encoding = \"ISO-8859-1\") as fp:\n",
    "                for line in fp:\n",
    "                    if line.startswith('Label##'):\n",
    "                        stance = \"Pro\"\n",
    "                        stance_line = line.replace(\"Label##\",\"\").split(\"-\")[0]\n",
    "                        if stance_line == \"c\":\n",
    "                            stance = \"Con\"                        \n",
    "                    if line.startswith('Line##'):\n",
    "                        arg = line.replace(\"Line##\",\"\")\n",
    "                        if file not in glob_data.keys():\n",
    "                            glob_data[file] = [topic,stance,stance,arg]\n",
    "                        else:\n",
    "                            glob_data[file][3] += (arg.strip() + \" \").strip()\n",
    "    \n",
    "    dr = pd.DataFrame.from_dict(glob_data, orient='index', columns=[\"text_topic\",\"text_source_stance\",\"text_source_domain\",\"text\"])\n",
    "    dr[\"text_title\"] = dr.index.values                            \n",
    "    return(dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_list = [\"data/reason/reason/abortion/*.rsn\"]\n",
    "topic = \"Abortion\"\n",
    "\n",
    "dr_abort = create_debate_df(path_list, topic)\n",
    "dr_abort.text_source_domain.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_list = [\"data/reason/reason/marijuana/*.rsn\"]\n",
    "topic = \"Marijuana\"\n",
    "\n",
    "dr_marijuana = create_debate_df(path_list, topic)\n",
    "dr_marijuana.text_source_domain.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_list = [\"data/reason/reason/gayRights/*.rsn\"]\n",
    "topic = \"Gay Marriage\"\n",
    "\n",
    "dr_gay = create_debate_df(path_list, topic)\n",
    "dr_gay.text_source_domain.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = pd.concat([dr_abort, dr_marijuana, dr_gay], ignore_index=True)\n",
    "dr = args_df.merge(dr, left_on=['topic'], right_on=['text_topic'])\n",
    "dr['text_processed'], dr['text_processed_size'] = preprocess_sentence(dr.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance computation - WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('data/model.bin', binary=True)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "tqdm.pandas()\n",
    "\n",
    "def wmd_computation(df):\n",
    "    df['wmd'] = df.progress_apply(lambda x: model.wmdistance(x.argument_processed.split(), x.text_processed.split()), axis=1)\n",
    "    df = df.query(\"wmd != \"+str(math.inf))\n",
    "    df = df.assign(wmd_norm = (df.wmd - df.wmd.min()) / (df.wmd.max() - df.wmd.min()))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments x Paragraph distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args_prg_dist = wmd_computation(args_prg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args_prg_dist.to_csv(\"../data/args_prg_dist.csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_prg_dist = pd.read_csv(\"data/args_prg_dist.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments x Documents distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampled_data = args_docs.sample(4000)\n",
    "#args_docs_dist = wmd_computation(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args_docs_dist.to_csv(\"../data/sampled_args_docs_dist.csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_docs_dist = pd.read_csv(\"data/sampled_args_docs_dist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = 0\n",
    "#idx = []\n",
    "#for txt in args_docs_dist.text:\n",
    "#    count += 1\n",
    "#    if ((topic == \"Marijuana\") and (\"medic\" in txt)):        \n",
    "#        idx.append(count)\n",
    "        \n",
    "#medic_marijuana_dist = args_docs_dist.iloc[idx]\n",
    "#abortion_dist = args_docs_dist[args_docs_dist.text_topic == \"Abortion\"]\n",
    "#args_docs_dist = pd.concat([medic_marijuana_dist, abortion_dist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments x Debate Reasons distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args_dr_dist = wmd_computation(dr)\n",
    "#args_dr_dist.to_csv(\"data/args_dr_dist.csv\", index_label=False)\n",
    "args_dr_dist = pd.read_csv(\"data/args_dr_dist.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Computation - Opinion Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.append(\"../Opinion-Distance/src/\")  \n",
    "from run_opinion_measure import get_opinion_distance, get_files_and_ground_truth, parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding opinion distance ../Opinion-Distance/dataset/CiviQ_Seanad//dist_mats/OD_embedding_strategy_word2vec.txt\n",
      "Representation time  2.161384344100952\n",
      "Opinion time 0.02850651741027832\n",
      "=========\n",
      "[0, 8, 0, 8, 8, 0, 8]\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allan/.local/lib/python3.7/site-packages/sklearn/cluster/_spectral.py:482: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "cmd_args = [\"--path\", \"../Opinion-Distance/dataset/CiviQ_Seanad/\", \"--embedding_strategy\", \"word2vec\", \"--semantic_threshold\", \"0.6\", \"--baselines\", \"False\"]\n",
    "args = parse_args(cmd_args)\n",
    "\n",
    "docs_path = \"tagme_docs\"\n",
    "full_path_files, ground_truth = None, None\n",
    "\n",
    "if \"CiviQ_Seanad\" in args.path:\n",
    "    Aspect = [\"Abolish\", \"Do not Abolish\", \"Democracy\"]\n",
    "    full_path_files, ground_truth = get_files_and_ground_truth(args.path, Aspect, docs_path=\"docs\")\n",
    "\n",
    "filename = (\"%s/dist_mats/OD_embedding_strategy_%s.txt\") % (args.path, args.embedding_strategy)\n",
    "fp = codecs.open(filename.replace(\"dist_mats\", \"results\"), \"w\")\n",
    "\n",
    "if args.baselines:\n",
    "    tf_idf_result(args.path + \"dist_mats/tf_idf.txt\")\n",
    "\n",
    "get_opinion_distance(filename=filename, filepath=fp, nfp_available=False, full_path_files=full_path_files, args=args, ground_truth=ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import permutation_test\n",
    "\n",
    "def weat(dist_df):\n",
    "\n",
    "    # mean distance of each text to each argument (argument - con/pro) set\n",
    "    sim_text_args = dist_df.groupby([\"text\",\"text_topic\",\"text_title\",\"text_source_domain\",\"argument_orient\",\"text_source_stance\"]).wmd_norm.agg(['mean']).reset_index()\n",
    "    sim_text_args = sim_text_args.rename(columns={\"mean\":\"mean_dist\"})\n",
    "\n",
    "    # difference of distance of each paragraph to the attribute sets\n",
    "    con_args = sim_text_args[sim_text_args.argument_orient == \"Con\"]\n",
    "    pro_args = sim_text_args[sim_text_args.argument_orient == \"Pro\"]\n",
    "    sim_text_args = pro_args.merge(con_args, on=[\"text\",\"text_topic\",\"text_title\",\"text_source_domain\",\"text_source_stance\"]).drop(columns=[\"argument_orient_x\", \"argument_orient_y\"])\n",
    "    sim_text_args = sim_text_args.rename(columns={\"mean_dist_x\":\"pro_dist\",\"mean_dist_y\":\"con_dist\"})\n",
    "    sim_text_args[\"diff_dist\"] = sim_text_args.pro_dist - sim_text_args.con_dist\n",
    "\n",
    "    topics = sim_text_args.text_topic.unique()\n",
    "\n",
    "    significance_test_array = []\n",
    "\n",
    "    for topic in topics:\n",
    "        topic_sources = sim_text_args[sim_text_args.text_topic == topic]\n",
    "        sources = topic_sources.text_source_domain.unique()\n",
    "        \n",
    "        for source in sources:\n",
    "            source_a = topic_sources[(topic_sources.text_source_domain == sources[0])]\n",
    "            source_b = topic_sources[(topic_sources.text_source_domain == sources[1])]\n",
    "\n",
    "            p_value = permutation_test(source_b.diff_dist, source_a.diff_dist,\n",
    "                                       method='approximate', num_rounds=1000)\n",
    "            \n",
    "            interpret = source_b.text_source_domain.iloc[0]+\"-Pro x \"+source_a.text_source_domain.iloc[0]+\"-Con\"\n",
    "            if source_b.diff_dist.mean() > source_a.diff_dist.mean():\n",
    "                interpret = source_b.text_source_domain.iloc[0]+\"-Con x \"+source_a.text_source_domain.iloc[0]+\"-Pro\"\n",
    "            \n",
    "            groundtruth = source_b.text_source_domain.iloc[0] + \"-\" + source_b.text_source_stance.iloc[0] + \" x \" + source_a.text_source_domain.iloc[0] +\"-\"+ source_a.text_source_stance.iloc[0]\n",
    "    \n",
    "        significance_test_array.append([topic, p_value, groundtruth, interpret, source_b.diff_dist.mean(), source_a.diff_dist.mean()])\n",
    "    \n",
    "    significance_test_df = pd.DataFrame(significance_test_array, columns=['topic', 'p_value', 'ground_truth', 'res_interpretation', 'cons_mean_dif', 'rwiki_mean_dif'])\n",
    "    \n",
    "    return(significance_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weat(args_prg_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weat(args_docs_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat(args_dr_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
