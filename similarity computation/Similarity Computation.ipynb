{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('word_embeddings/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_json = pd.read_json(\"../crawlers/procon/wiki.json\")\n",
    "args_json = pd.read_json(\"../crawlers/procon/procon.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format DataFrame rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_args_row(row):\n",
    "    df = row.apply(pd.Series).T\n",
    "    df.url = df.url[0]\n",
    "    df.topic = df.topic[0]\n",
    "    return(df)\n",
    "\n",
    "arg_df_series = args_json.apply(format_args_row, axis=1)\n",
    "args_df = pd.concat(arg_df_series.values.tolist(),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_wiki_row(row):\n",
    "    df = row.apply(pd.Series).T\n",
    "    df.source = df.source[0]\n",
    "    df.title = df.title[0]\n",
    "    df.topic = df.topic[0]\n",
    "    return(df)\n",
    "\n",
    "wiki_df_series = wiki_json.apply(format_wiki_row, axis=1)\n",
    "wiki_df = pd.concat(wiki_df_series.values.tolist(),ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe which will store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmd_df = args_df.merge(wiki_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wmd_df = wmd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase and remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/allan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords') \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess_sentence(str_array):\n",
    "    new_sentences = []\n",
    "    sentences_size = []\n",
    "    for text in str_array:\n",
    "        new_sentence = \"\"\n",
    "        for word in text.split():\n",
    "            if word not in stop_words:\n",
    "                new_sentence += \" \" + word.lower()\n",
    "        sentences_size.append(new_sentence.count(\" \"))\n",
    "        new_sentences.append(new_sentence.strip())\n",
    "        \n",
    "    return(new_sentences, sentences_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmd_df['argument_processed'], wmd_df['argument_processed_size']  = preprocess_sentence(wmd_df.argument)\n",
    "wmd_df['content_processed'], wmd_df['content_processed_size'] = preprocess_sentence(wmd_df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wmd_df = wmd_df.assign(argument_processed = preprocess_sentence(wmd_df.argument),\n",
    "#                      content_processed = preprocess_sentence(wmd_df.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add argument orientation information  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orientation(pro_con):\n",
    "    if \"Pro\" in pro_con:\n",
    "        return \"Pro\"\n",
    "    return \"Con\"\n",
    "\n",
    "wmd_df = wmd_df.assign(arg_orient = wmd_df.apply(lambda row: orientation(row.pro_con), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rows which either the content_processed or argument_processed is too short\n",
    "\n",
    "Our goal is computing the similarity between an idea (paragraph) and an argument. However, some rows contain only a few words (sometimes only one word), which are not enough for expressing one idea. Thus, we define a variable for determining the minimum size of a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_min_size = 5\n",
    "\n",
    "wmd_df = wmd_df.query(\"content_processed_size > \" + str(paragraph_min_size)).query(\"argument_processed_size > \" + str(paragraph_min_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the WMD between arguments and each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allan/anaconda2/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      " 69%|██████▊   | 30484/44426 [16:45<06:37, 35.03it/s] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "wmd_df['wmd'] = wmd_df.progress_apply(lambda x: model.wmdistance(x.argument_processed.split(), x.content_processed.split()), axis=1)\n",
    "wmd_df.to_csv ('wmd_distances.csv', index = None, header=True)\n",
    "#wmd_df = pd.read_csv(\"wmd_distances.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmd_df = wmd_df.assign(wmd_sim = (wmd_df.wmd - wmd_df.wmd.min()) / (wmd_df.wmd.max() - wmd_df.wmd.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT for documents (name it better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similarity between paragraph and a pro/con set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_paragraph_args = wmd_df.groupby(['topic','source','content',\"arg_orient\"]).wmd_sim.agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_paragraph_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_paragraph_args.query(\"topic == 'Abortion'\").query(\"source == 'conservapedia'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toy example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = {'content':[\"pa1\",\"pa2\",\"pa1\",\"pa2\",\"pa1\",\"pa2\",\"pa1\",\"pa2\",\"pa1\",\"pa2\"],\n",
    "             'pro_con':[\"c1\",\"c1\",\"c2\",\"c2\",\"p1\",\"p1\",\"p2\",\"p2\",\"p3\",\"p3\"],\n",
    "             'source':[\"con\",\"con\",\"con\",\"con\",\"con\",\"con\",\"con\",\"con\",\"con\",\"con\"],\n",
    "             'topic':[\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"],\n",
    "             'wmd_sim':[0.8, 0.8, 0.9, 0.7, 0.6, 0.8, 0.8, 0.9, 0.7, 0.6]} \n",
    "\n",
    "fake_data_2 = {'content':[\"pa1\",\"pa2\",\"pa3\",\"pa1\",\"pa2\",\"pa3\",\"pa1\",\"pa2\",\"pa3\",\"pa1\",\"pa2\",\"pa3\",\"pa1\",\"pa2\",\"pa3\"],\n",
    "             'pro_con':[\"c1\",\"c1\",\"c1\",\"c2\",\"c2\",\"c2\",\"p1\",\"p1\",\"p1\",\"p2\",\"p2\",\"p2\",\"p3\",\"p3\",\"p3\"],\n",
    "             'source':[\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\"],\n",
    "             'topic':[\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"],\n",
    "             'wmd_sim':[0.8, 0.8, 0.9, 0.7, 0.6, 0.8, 0.8, 0.9, 0.7, 0.6, 0.8, 0.8, 0.9, 0.7, 0.6]} \n",
    "\n",
    "fake_data_3 = {'content':[\"pa1\",\"pa2\",\"pa1\",\"pa2\",\"pa1\",\"pa2\",\"pa1\",\"pa2\",\"pa1\",\"pa2\",\"pa1\",\"pa2\",\"pa3\",\"pa1\",\"pa2\",\"pa3\",\"pa1\",\"pa2\",\"pa3\",\"pa1\",\"pa2\",\"pa3\",\"pa1\",\"pa2\",\"pa3\"],\n",
    "             'pro_con':[\"Con 1\",\"Con 1\",\"Con 2\",\"Con 2\",\"Pro 1\",\"Pro 1\",\"Pro 2\",\"Pro 2\",\"Pro 3\",\"Pro 3\",\"Con 1\",\"Con 1\",\"Con 1\",\"Con 2\",\"Con 2\",\"Con 2\",\"Pro 1\",\"Pro 1\",\"Pro 1\",\"Pro 2\",\"Pro 2\",\"Pro 2\",\"Pro 3\",\"Pro 3\",\"Pro 3\"],\n",
    "             'source':[\"con\",\"con\",\"con\",\"con\",\"con\",\"con\",\"con\",\"con\",\"con\",\"con\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\",\"rat\"],\n",
    "             'topic':[\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"],\n",
    "             'wmd_sim':[0.8, 0.8, 0.9, 0.7, 0.6, 0.8, 0.8, 0.9, 0.7, 0.6,0.8, 0.8, 0.9, 0.7, 0.6, 0.8, 0.8, 0.9, 0.7, 0.6, 0.8, 0.8, 0.9, 0.7, 0.6]} \n",
    "\n",
    "fake_wmd_df = pd.DataFrame(fake_data_3) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
